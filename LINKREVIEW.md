# LinkReview


| Topic | Title | Year | Authors | Paper | Code |  Summary                 |
| :--- | :--- | :---: | :--- | :---: | :---: | :--- |
| Neural Operators - Theory | Operator Learning: Algorithms and Analysis | 2024 | Nikola B. Kovachki, Samuel Lanthaler, Andrew M. Stuart | [arXiv:2402.15715](https://arxiv.org/abs/2402.15715) | — | Comprehensive review of approximation theory for neural operators mapping between infinite-dimensional Banach spaces. Covers  fundamental algorithms (DeepONet, FNO, PCA-Net, Random Features) with theoretical analysis including universal approximation theorems, complexity bounds, and curse of dimensionality |
| POD-based Comparison & Extensions | A Comprehensive and Fair Comparison of Two Neural Operators (with Practical Extensions) Based on FAIR Data | 2022 | Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, George Em Karniadakis | [arXiv:2111.05512](https://arxiv.org/abs/2111.05512) | [GitHub](https://github.com/lu-group/deeponet-fno) | Benchmark comparison of DeepONet and FNO across 16 diverse tasks. Introduces extensions: POD-DeepONet, improved boudary condition handling, and FNO variants for complex domains. DeepONet proves more robust to noise. POD concept - precomputed POD basis as the trunk net|
| POD | Neural-POD: A Plug-and-Play Neural Operator Framework for Infinite-Dimensional Functional Nonlinear Proper Orthogonal Decomposition | 2026 | Changhong Mou, Binghang Lu, Guang Lin| [ArXiv:2602.15632](https://arxiv.org/pdf/2602.15632v1) | — | **BEST** Introducing neural operator framework that constructs nonlinear, orthogonal  basis functions in infinite-dimensional space using NNs. As a result - basis construction as a sequence of residual minimization problems solved through neural network training  |
| POD-based Nonlinear Reduction | Nonlinear Model Reduction for Operator Learning | 2024 | Hamidreza Eivazi, Stefan Wittek, Andreas Rausch | [arXiv:2403.18735](https://arxiv.org/abs/2403.18735) | [GitHub](https://github.com/HamidrezaEiv/KPCA-DeepONet) |**BEST** Proposes KPCA-DeepONet combining kernel principal component analysis with DeepONet for nonlinear model order reduction. Extends POD-based approaches with nonlinear dimensionality reduction, achieving superior performance on operator learning benchmarks. |
| POD-based Ensemble & Mixture-of-Experts | Ensemble and Mixture-of-Experts DeepONets For Operator Learning | 2025 | Ramansh Sharma, Varun Shankar | [arXiv:2405.11907](https://arxiv.org/pdf/2405.11907) | [GitHub](https://github.com/rsmath/ensemble-deeponet) | **INTERESTING** Introduces ensemble DeepONets with multiple distinct trunk networks and Partition-of-Unity Mixture-of-Experts (PoU-MoE) architecture. Achieves 2-4x lower errors compared to standard DeepONets. Combines POD modes with MoE for improved expressivity and spatial locality. |
| POD-based | PODNO: Proper Orthogonal Decomposition Neural Operators | 2025 | Zilan Cheng, Zhongjian Wang, Li-Lian Wang, Mejdi Azaiez| [ArXiv:2504.18513](https://arxiv.org/pdf/2504.18513) | — | Proper Orthogonal Decomposition Neural Operators (PODNO) - a neural operator architecture that replaces the Fourier transform in FNO with data-driven POD basis functions to solve PDEs dominated by high-frequency oscillatory components. Unlike FNO which truncates high-frequency modes, PODNO computes an optimal basis from training snapshots that captures both low and high-frequency. Cool GSO. PODNO outperforms FNO on high-frequency probems|
| PI-DeepONets | Learning the solution operator of parametric partial differential equations with physics-informed DeepONets | 2021 | Sifan Wang, Hanwen Wang, Paris Perdikaris | [DOI](https://www.science.org/doi/full/10.1126/sciadv.abi8605) | — |  physics-informed DeepONet - framework for rapidly predicting the solution of various types of parametric PDEs, including basing the outputs of DeepONets toward physically consistent predictions|
| PI-DeepONets | Separable Physics-Informed DeepONet: Breaking the Curse of Dimensionality in Physics-Informed Machine Learning | 2024 | Luis Mandla, Somdatta Goswami, Lena Lamberts, Tim Ricken | [arXiv:2407.15887](https://arxiv.org/pdf/2407.15887) | — |  Aims to solve curse of dimensionality by introducing separable physics-informed DeepONet (Sep-PI-DeepONet). Factorizing coordinates and separate sub-networks for each one-dimensional coordinate - from $O(Nn^d) \text{ to } O(Nd)$ for N input functions|
| One-shot Learning for PDEs | One-shot learning for solution operators of partial differential equations | 2025 | Anran Jiao, Haiyang He, Rishikesh Ranade, Jay Pathak & Lu Lu  | [DOI](https://www.nature.com/articles/s41467-025-63076-z) | [GitHub](https://github.com/lu-group/one-shot-pde) | One-shot task for PDE solving (PDE solution operators from only one data point) - predict solutions of new input functions via mesh-based fixed-point iteration or meshfree neural-network based approaches. 3 approaches - fixed-point iteration (FPI), local-solution-operator-informed neural network (LOINN), LOINN with correction (cLOINN). |
| Domain decomposition DeepONets| DD-DeepONet: Domain decomposition and DeepONet for solving partial differential equations in three application scenarios | 2025 | Bo Yang, Xingquan Li, Jie Zhao, Ying Jiang | Paper | — |  Introducing framework for decomposing complex geometries into simple structures and vice versa. Using DDM - breaks problems in several smaller problems|
| Efficiency & Acceleration | DeepONet Augmented by Randomized Neural Networks for Efficient Operator Learning in PDEs | 2025 | Zhaoxi Jiang, Fei Wang | [ArXiv:2503.00317](https://arxiv.org/pdf/2503.00317) | — |  Integrating randomized neural networks(RaNNs) with DeepONet architecture to balance accuracy and efficiency. Also developing PI-RaNN-DeepONets |