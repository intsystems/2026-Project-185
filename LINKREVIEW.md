# LinkReview


| Topic | Title | Year | Authors | Paper | Code |  Summary |
| :--- | :--- | :---: | :--- | :---: | :---: | :--- |
| Neural Operators - Theory | Operator Learning: Algorithms and Analysis | 2024 | Nikola B. Kovachki, Samuel Lanthaler, Andrew M. Stuart | [arXiv:2402.15715](https://arxiv.org/abs/2402.15715) | — | Comprehensive review of approximation theory for neural operators mapping between infinite-dimensional Banach spaces. Covers  fundamental algorithms (DeepONet, FNO, PCA-Net, Random Features) with theoretical analysis including universal approximation theorems, complexity bounds, and curse of dimensionality |
| POD-based Comparison & Extensions | A Comprehensive and Fair Comparison of Two Neural Operators (with Practical Extensions) Based on FAIR Data | 2022 | Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, George Em Karniadakis | [arXiv:2111.05512](https://arxiv.org/abs/2111.05512) | [GitHub](https://github.com/lu-group/deeponet-fno) | Benchmark comparison of DeepONet and FNO across 16 diverse tasks. Introduces extensions: POD-DeepONet, improved boudary condition handling, and FNO variants for complex domains. DeepONet proves more robust to noise. POD concept - precomputed POD basis as the trunk net|
| POD | Neural-POD: A Plug-and-Play Neural Operator Framework for Infinite-Dimensional Functional Nonlinear Proper Orthogonal Decomposition | 2026 | Changhong Mou, Binghang Lu, Guang Lin| [ArXiv:2602.15632](https://arxiv.org/pdf/2602.15632v1) | — | **BEST** Introducing neural operator framework that constructs nonlinear, orthogonal  basis functions in infinite-dimensional space using NNs. As a result - basis construction as a sequence of residual minimization problems solved through neural network training  |
| POD-based Nonlinear Reduction | Nonlinear Model Reduction for Operator Learning | 2024 | Hamidreza Eivazi, Stefan Wittek, Andreas Rausch | [arXiv:2403.18735](https://arxiv.org/abs/2403.18735) | [GitHub](https://github.com/HamidrezaEiv/KPCA-DeepONet) |**BEST** Proposes KPCA-DeepONet combining kernel principal component analysis with DeepONet for nonlinear model order reduction. Extends POD-based approaches with nonlinear dimensionality reduction, achieving superior performance on operator learning benchmarks. |
| POD-based Ensemble & Mixture-of-Experts | Ensemble and Mixture-of-Experts DeepONets For Operator Learning | 2025 | Ramansh Sharma, Varun Shankar | [arXiv:2405.11907](https://arxiv.org/pdf/2405.11907) | [GitHub](https://github.com/rsmath/ensemble-deeponet) | **INTERESTING** Introduces ensemble DeepONets with multiple distinct trunk networks and Partition-of-Unity Mixture-of-Experts (PoU-MoE) architecture. Achieves 2-4x lower errors compared to standard DeepONets. Combines POD modes with MoE for improved expressivity and spatial locality. |
| POD-based | PODNO: Proper Orthogonal Decomposition Neural Operators | 2025 | Zilan Cheng, Zhongjian Wang, Li-Lian Wang, Mejdi Azaiez| [ArXiv:2504.18513](https://arxiv.org/pdf/2504.18513) | — | Proper Orthogonal Decomposition Neural Operators (PODNO) - a neural operator architecture that replaces the Fourier transform in FNO with data-driven POD basis functions to solve PDEs dominated by high-frequency oscillatory components. Unlike FNO which truncates high-frequency modes, PODNO computes an optimal basis from training snapshots that captures both low and high-frequency. Cool GSO. PODNO outperforms FNO on high-frequency probems|
| PI-DeepONets | Learning the solution operator of parametric partial differential equations with physics-informed DeepONets | 2021 | Sifan Wang, Hanwen Wang, Paris Perdikaris | [DOI](https://www.science.org/doi/full/10.1126/sciadv.abi8605) | [GitHub](https://github.com/PredictiveIntelligenceLab/Physics-informed-DeepONets) |  physics-informed DeepONet - framework for rapidly predicting the solution of various types of parametric PDEs, including biasing the outputs of DeepONets toward physically consistent predictions|
| PI-DeepONets | Separable Physics-Informed DeepONet: Breaking the Curse of Dimensionality in Physics-Informed Machine Learning | 2024 | Luis Mandla, Somdatta Goswami, Lena Lamberts, Tim Ricken | [arXiv:2407.15887](https://arxiv.org/pdf/2407.15887) | [GitHub](https://github.com/lmandl/separable-PI-DeepONet) |  Aims to solve curse of dimensionality by introducing separable physics-informed DeepONet (Sep-PI-DeepONet). Factorizing coordinates and separate sub-networks for each one-dimensional coordinate - from $O(Nn^d) \text{ to } O(Nd)$ for N input functions|
| One-shot Learning for PDEs | One-shot learning for solution operators of partial differential equations | 2025 | Anran Jiao, Haiyang He, Rishikesh Ranade, Jay Pathak & Lu Lu  | [DOI](https://www.nature.com/articles/s41467-025-63076-z) | [GitHub](https://github.com/lu-group/one-shot-pde) | One-shot task for PDE solving (PDE solution operators from only one data point) - predict solutions of new input functions via mesh-based fixed-point iteration or meshfree neural-network based approaches. 3 approaches - fixed-point iteration (FPI), local-solution-operator-informed neural network (LOINN), LOINN with correction (cLOINN). |
| Domain decomposition DeepONets| DD-DeepONet: Domain decomposition and DeepONet for solving partial differential equations in three application scenarios | 2025 | Bo Yang, Xingquan Li, Jie Zhao, Ying Jiang | Paper | — |  Introducing framework for decomposing complex geometries into simple structures and vice versa. Using DDM - breaks problems in several smaller problems|
| Efficiency & Acceleration | DeepONet Augmented by Randomized Neural Networks for Efficient Operator Learning in PDEs | 2025 | Zhaoxi Jiang, Fei Wang | [ArXiv:2503.00317](https://arxiv.org/pdf/2503.00317) | [GitHub](https://github.com/Centrum-IntelliPhysics/DeepONet-Efficient-Training-with-Random-Sampling) |  Integrating randomized neural networks(RaNNs) with DeepONet architecture to balance accuracy and efficiency. Also developing PI-RaNN-DeepONets |
| POD | Neural Basis Functions for Accelerating Solutions to High Mach Euler Equations | 2022 | David Witman, Alexander New, Hicham Alkandry, Honest Mrema | [ArXiv:2208.01687](https://arxiv.org/pdf/2208.01687) | — |  Neural Basis Functions (NFS) = POD in DeepONets - goes from Finite Element Method (FEM) approach and uses a set of approximating neural networks to learn the underlying bases and explicit unknowns of the governing equations |
| PI | TARGETED DIGITAL TWIN VIA FLOW MAP LEARNING AND ITS APPLICATION TO FLUID DYNAMICS | 2025 | QIFAN CHEN , ZHONGSHU XU ,JINJIN ZHANG , DONGBIN XIU | [ArXiv:2510.07549](https://arxiv.org/pdf/2510.07549) | — |  targeting digital twin - directly models the dynamics of specific quantities of interest by learning from short simulation bursts of a full digital twin. Not exactly what needed|
| PI | CANONICAL AND NONCANONICAL HAMILTONIAN OPERATOR INFERENCE | 2023 | ANTHONY GRUBER, IRINA TEZAUR | [ArXiv:2304.06262](https://arxiv.org/pdf/2304.06262) | — | **Interesting** Structure-preserving model reduction of canonical and non-canonical Hamiltonian systems. Making POD of H, math may be useful for physical systems|
| PI | Hamiltonian Learning using Machine Learning Models Trained with Continuous Measurements | 2025 | Kris Tucker, Amit Kiran Rege, Conor Smith, Claire Monteleoni, Tameem Albash | [ArXiv:2404.05526](https://arxiv.org/pdf/2404.05526) | — |  Qubit simulation, using supervised and unsupervised learning. Second - mapping H parameters to a measurement record using integrator with RNN. Interesting connection of LSTM and solvers(integrators) |
| PI | Physics-informed machine learning | 2021 | George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, Liu Yang| [DoI](https://www.nature.com/articles/s42254-021-00314-5?utm_source=researchgate.net&utm_medium=article) | — |  Foundation principals of physics-informed ML systems. Core idea - integrating physical laws(PDEs) directly into the training process of NNs, which allows models to learn from both data and underlying mathematical principles |
| PI-ROM | Physics-informed non-intrusive reduced-order modeling of parameterized dynamical systems | 2025 | Himanshu Dave, Leo Cotteleer, Alessandro Parente | [DOI:10.1016/j.cma.2025.118045](https://doi.org/10.1016/j.cma.2025.118045) | — | Physics-informed non-intrusive ROM framework for parametrized PDEs. Uses POD decomposition to extract POD modes from high-fidelity training data. Outperforms vanilla POD + ANN ROM by one order of magnitude in relative error. Includes self-calibration technique for loss function hyperparameter tuning. |
| Neural POD | OrthoSolver: A Neural Proper Orthogonal Decomposition Solver For PDEs | 2026 | Anonymous authors (Paper under double-blind review) | [OpenReview](https://openreview.net/forum?id=9OOmlDrEfn) | — | OrthoSolver - a neural POD framework that generalizes POD's energy-maximization principle from an information-theoretic perspectve. Maximizes mutual information between modes and data rather than just variance. Outperforms Neural-POD with different constraints.|
| POD | Geometry-agnostic model reduction with GNN-generated reduced POD bases and boosted PGD enrichment for (non)linear structural elastodynamics | 2025 | Victor Matray, David Néron, Frédéric Feyel, Faisal Amlani | [HALscience](https://hal.science/hal-05288454) | Code |  hybrid method that combines GNN with POD to create reduced-order models. Uses a Grassmannian distance as a training objective and introduces a "Boosted PGD" enrichment mechanism to improve accuracy for both linear and nonlinear structural dynamics problems. Good geometry interpretation |