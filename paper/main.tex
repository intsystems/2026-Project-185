\documentclass{article}

\PassOptionsToPackage{numbers, sort, compress}{natbib}
\usepackage[main,final]{neurips_2025}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}

%%%

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{manyfoot}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{listings}

\newtheorem{theorem}{Theorem} % continuous numbers
%%\newtheorem{theorem}{Theorem}[section] % sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
\newtheorem{lemma}{Lemma}% 
%%\newtheorem{proposition}{Proposition} % to get separate numbers for theorem and proposition etc.

\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

%%%


\title{TEMPO: EM-Based Mixture-of-PODs for Operator Learning}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Rodion Akinzhala\\
  MIPT\\
  Moscow, Russia\\
  \And
  Alexander Terentyev (Consultant)\\
  MIPT\\
  Moscow, Russia\\
  \And
  Vadim Strijov (Advisor)\\
  MIPT\\
  Moscow, Russia\\
}


\begin{document}


\maketitle

\begin{abstract}
    Deep Operator Networks (DeepONet) learn mappings between function spaces through branch and trunk neural networks. POD-DeepONet improves upon this by replacing the trunk network with Proper Orthogonal Decomposition bases derived from solution snapshots, achieving superior accuracy and training efficiency. However, existing POD-DeepONet methods employ a single global basis across all parameters, which proves insufficient when solutions exhibit regime-dependent behavior. We propose an adaptive POD-DeepONet framework combining Expectation-Maximization clustering with multiple regime-specific bases. Gaussian Mixture Models automatically identify distinct dynamical regimes by partitioning solution snapshots. For each regime, we construct locally optimal POD bases that capture regime-specific features, where modern Neural-POD approaches can be employed for basis construction. The architecture features multiple cluster-conditioned trunks with probabilistic mixture weights for smooth regime transitions, while branch networks predict regime-specific coefficients. This framework enables automatic regime discovery, achieves superior local approximation through specialized bases, reduces dimensionality requirements per regime, and provides natural uncertainty quantification. The probabilistic formulation ensures continuous predictions across regime boundaries without manual domain partitioning. We validate the approach on PDEs exhibiting multi-scale and regime-dependent phenomena, demonstrating significant improvements over standard POD-DeepONet for complex operators.
\end{abstract}

\textbf{Keywords:} Operator learning, DeepONet, Proper Orthogonal Decomposition, Expectation-Maximization, Gaussian mixture models, Neural-POD

\section{Introduction}\label{sec:intro}

TODO

\textbf{Contributions.} Our contributions can be summarized as follows:
\begin{itemize}
    \item We present...
    \item We demonstrate the validity of our theoretical results through empirical studies...
    \item We highlight the implications of our findings for...
\end{itemize}

\textbf{Outline.} The rest of the paper is organized as follows...

\section{Related Work}\label{sec:rw}

\textbf{Topic \#1.}
TODO

\textbf{Topic \#2.}
TODO

\section{Preliminaries}\label{sec:prelim}

\subsection{General notation}

In this section, we introduce the general notation used in the rest of the paper and the basic assumptions. 

\subsection{Assumptions} 

TODO

\section{Method}\label{sec:method}

\section{Experiments}\label{sec:exp}

To verify the theoretical estimates obtained, we conducted a detailed empirical study...

\section{Discussion}\label{sec:disc}

TODO

\section{Conclusion}\label{sec:concl}

TODO


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{unsrtnat}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\section{Appendix / supplemental material}\label{app}

\subsection{Additional experiments / Proofs of Theorems}\label{app:exp}

TODO

\end{document}